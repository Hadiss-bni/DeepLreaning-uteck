{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework-series1-N5-LRfinderCallback.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9uFpxnGAXnmHFXnB0lr+/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hadiss-bni/DeepLreaning-UtechAcademy/blob/master/Homework_series1_N5_LRfinderCallback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1tjGLbUBcEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "a66cab3f-8eac-4932-d459-273dba93100c"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
        "!mkdir dataset\n",
        "!wget https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat -P dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-18 18:44:59--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 929 [text/plain]\n",
            "Saving to: ‘dataset.py.4’\n",
            "\n",
            "dataset.py.4        100%[===================>]     929  --.-KB/s    in 0s      \n",
            "\n",
            "2020-09-18 18:45:00 (68.1 MB/s) - ‘dataset.py.4’ saved [929/929]\n",
            "\n",
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "--2020-09-18 18:45:00--  https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat [following]\n",
            "--2020-09-18 18:45:01--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3989009 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘dataset/Data_hoda_full.mat.4’\n",
            "\n",
            "Data_hoda_full.mat. 100%[===================>]   3.80M  17.1MB/s    in 0.2s    \n",
            "\n",
            "2020-09-18 18:45:02 (17.1 MB/s) - ‘dataset/Data_hoda_full.mat.4’ saved [3989009/3989009]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE9NvZiSBHXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Import libraries and modules\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "import numpy as np\n",
        "from dataset import load_hoda\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(123)  # for reproducibility\n",
        "\n",
        "# Load pre-shuffled HODA data into train and test sets\n",
        "x_train_original, y_train_original, x_test_original, y_test_original = load_hoda(\n",
        "                                                                        training_sample_size=3500,\n",
        "                                                                        test_sample_size=400,size=28)\n",
        "\n",
        "# Preprocess input data\n",
        "''' 3.1: input data in numpy array format'''\n",
        "x_train = np.array(x_train_original)\n",
        "x_test = np.array(x_test_original)\n",
        "'''3.2 normalize our data values to the range [0, 1]'''\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Reshape to original image shape (n x 784)  ==> (n x 28 x 28 x 1)\n",
        "x_train = x_train.reshape(-1,28,28,1)\n",
        "x_test = x_test.reshape(-1,28,28,1)\n",
        "\n",
        "\n",
        "# 4. Preprocess class labels\n",
        "y_train = keras.utils.to_categorical(y_train_original, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test_original, num_classes=10)\n",
        "\n",
        "\n",
        "# test and validation set\n",
        "x_val = x_test[:200]\n",
        "x_test = x_test[200:]\n",
        "y_val = y_test[:200]\n",
        "y_test = y_test[200:]\n",
        "\n",
        "# 5. Define model architecture\n",
        "model = Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# 6. Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ihZ7QD8BXk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    \"\"\"\n",
        "    Up-to date version: https://github.com/WittmannF/LRFinder\n",
        "    Example of usage:\n",
        "        from keras.models import Sequential\n",
        "        from keras.layers import Flatten, Dense\n",
        "        from keras.datasets import fashion_mnist\n",
        "        !git clone https://github.com/WittmannF/LRFinder.git\n",
        "        from LRFinder.keras_callback import LRFinder\n",
        "        # 1. Input Data\n",
        "        (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "        mean, std = X_train.mean(), X_train.std()\n",
        "        X_train, X_test = (X_train-mean)/std, (X_test-mean)/std\n",
        "        # 2. Define and Compile Model\n",
        "        model = Sequential([Flatten(),\n",
        "                            Dense(512, activation='relu'),\n",
        "                            Dense(10, activation='softmax')])\n",
        "        model.compile(loss='sparse_categorical_crossentropy', \\\n",
        "                      metrics=['accuracy'], optimizer='sgd')\n",
        "        # 3. Fit using Callback\n",
        "        lr_finder = LRFinder(min_lr=1e-4, max_lr=1)\n",
        "        model.fit(X_train, y_train, batch_size=128, callbacks=[lr_finder], epochs=2)\n",
        "    \"\"\"\n",
        "    def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None, \n",
        "                 reload_weights=True, batches_lr_update=5):\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.mom = mom\n",
        "        self.reload_weights = reload_weights\n",
        "        self.batches_lr_update = batches_lr_update\n",
        "        if stop_multiplier is None:\n",
        "            self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n",
        "                                                       # 10 if mom=0\n",
        "        else:\n",
        "            self.stop_multiplier = stop_multiplier\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        p = self.params\n",
        "        try:\n",
        "            n_iterations = p['epochs']*p['samples']//p['batch_size']\n",
        "        except:\n",
        "            n_iterations = p['steps']*p['epochs']\n",
        "            \n",
        "        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n",
        "                                           num=n_iterations//self.batches_lr_update+1)\n",
        "        self.losses=[]\n",
        "        self.iteration=0\n",
        "        self.best_loss=0\n",
        "        if self.reload_weights:\n",
        "            self.model.save_weights('tmp.hdf5')\n",
        "        \n",
        "    \n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        loss = logs.get('loss')\n",
        "        \n",
        "        if self.iteration!=0: # Make loss smoother using momentum\n",
        "            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n",
        "        \n",
        "        if self.iteration==0 or loss < self.best_loss: \n",
        "                self.best_loss = loss\n",
        "                \n",
        "        if self.iteration%self.batches_lr_update==0: # Evaluate each lr over 5 epochs\n",
        "            \n",
        "            if self.reload_weights:\n",
        "                self.model.load_weights('tmp.hdf5')\n",
        "          \n",
        "            lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n",
        "            K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "            self.losses.append(loss)            \n",
        "\n",
        "        if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n",
        "            self.model.stop_training = True\n",
        "                \n",
        "        self.iteration += 1\n",
        "    \n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.reload_weights:\n",
        "                self.model.load_weights('tmp.hdf5')\n",
        "                \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.learning_rates[:len(self.losses)], self.losses)\n",
        "        plt.xlabel(\"Learning Rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xscale('log')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49jUk7iiGlYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43fc3cea-4146-46c2-a0b9-1e7ba23976dc"
      },
      "source": [
        "# 7. Fit model on training data\n",
        "callback = LRFinder(min_lr=1e-10,max_lr=1e+1)\n",
        "history = model.fit(x_train, y_train, epochs=200,batch_size=265,\n",
        "           validation_data = (x_val, y_val),callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3034 - accuracy: 0.1137 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3018 - accuracy: 0.1120 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3020 - accuracy: 0.1163 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2996 - accuracy: 0.1154 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3019 - accuracy: 0.1177 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3001 - accuracy: 0.1197 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 2.3030 - accuracy: 0.1186 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3033 - accuracy: 0.1143 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3026 - accuracy: 0.1146 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3024 - accuracy: 0.1174 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3053 - accuracy: 0.1149 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3007 - accuracy: 0.1131 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1120 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3017 - accuracy: 0.1149 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3026 - accuracy: 0.1166 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3036 - accuracy: 0.1211 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3013 - accuracy: 0.1197 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3039 - accuracy: 0.1157 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3038 - accuracy: 0.1211 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3023 - accuracy: 0.1094 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3034 - accuracy: 0.1194 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3056 - accuracy: 0.1049 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2995 - accuracy: 0.1157 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3021 - accuracy: 0.1117 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3018 - accuracy: 0.1146 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3024 - accuracy: 0.1149 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3029 - accuracy: 0.1169 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3042 - accuracy: 0.1166 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3029 - accuracy: 0.1166 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3016 - accuracy: 0.1166 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3018 - accuracy: 0.1080 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3017 - accuracy: 0.1209 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3052 - accuracy: 0.1094 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3020 - accuracy: 0.1217 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3028 - accuracy: 0.1100 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3026 - accuracy: 0.1191 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3008 - accuracy: 0.1200 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3005 - accuracy: 0.1220 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3014 - accuracy: 0.1126 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3021 - accuracy: 0.1214 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3024 - accuracy: 0.1129 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3030 - accuracy: 0.1117 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3014 - accuracy: 0.1046 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3017 - accuracy: 0.1163 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3016 - accuracy: 0.1214 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2997 - accuracy: 0.1191 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3027 - accuracy: 0.1129 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3003 - accuracy: 0.1174 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3015 - accuracy: 0.1200 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3034 - accuracy: 0.1203 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3000 - accuracy: 0.1183 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3032 - accuracy: 0.1171 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.2995 - accuracy: 0.1194 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3033 - accuracy: 0.1131 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3028 - accuracy: 0.1149 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3011 - accuracy: 0.1083 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3017 - accuracy: 0.1169 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3044 - accuracy: 0.1109 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3015 - accuracy: 0.1166 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3022 - accuracy: 0.1129 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3033 - accuracy: 0.1097 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3013 - accuracy: 0.1203 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3016 - accuracy: 0.1160 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3027 - accuracy: 0.1111 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3022 - accuracy: 0.1169 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3021 - accuracy: 0.1137 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3011 - accuracy: 0.1091 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3033 - accuracy: 0.1203 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3022 - accuracy: 0.1160 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3030 - accuracy: 0.1171 - val_loss: 2.2946 - val_accuracy: 0.1600\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3009 - accuracy: 0.1157 - val_loss: 2.2946 - val_accuracy: 0.1600\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3024 - accuracy: 0.1134 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3023 - accuracy: 0.1189 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3012 - accuracy: 0.1146 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3045 - accuracy: 0.1077 - val_loss: 2.2945 - val_accuracy: 0.1600\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3018 - accuracy: 0.1186 - val_loss: 2.2946 - val_accuracy: 0.1600\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3046 - accuracy: 0.1149 - val_loss: 2.2946 - val_accuracy: 0.1600\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3009 - accuracy: 0.1254 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1109 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3028 - accuracy: 0.1166 - val_loss: 2.2944 - val_accuracy: 0.1600\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3023 - accuracy: 0.1146 - val_loss: 2.2944 - val_accuracy: 0.1600\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3008 - accuracy: 0.1140 - val_loss: 2.2945 - val_accuracy: 0.1600\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3013 - accuracy: 0.1166 - val_loss: 2.2946 - val_accuracy: 0.1600\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3007 - accuracy: 0.1174 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3018 - accuracy: 0.1160 - val_loss: 2.2941 - val_accuracy: 0.1600\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3002 - accuracy: 0.1220 - val_loss: 2.2941 - val_accuracy: 0.1600\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1191 - val_loss: 2.2943 - val_accuracy: 0.1600\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3014 - accuracy: 0.1129 - val_loss: 2.2945 - val_accuracy: 0.1600\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3023 - accuracy: 0.1149 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.3021 - accuracy: 0.1200 - val_loss: 2.2935 - val_accuracy: 0.1600\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3006 - accuracy: 0.1154 - val_loss: 2.2936 - val_accuracy: 0.1600\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3004 - accuracy: 0.1143 - val_loss: 2.2939 - val_accuracy: 0.1600\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3008 - accuracy: 0.1206 - val_loss: 2.2943 - val_accuracy: 0.1600\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3024 - accuracy: 0.1191 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3026 - accuracy: 0.1117 - val_loss: 2.2923 - val_accuracy: 0.1600\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2985 - accuracy: 0.1154 - val_loss: 2.2927 - val_accuracy: 0.1600\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3008 - accuracy: 0.1200 - val_loss: 2.2932 - val_accuracy: 0.1600\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3010 - accuracy: 0.1163 - val_loss: 2.2938 - val_accuracy: 0.1600\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3011 - accuracy: 0.1186 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3019 - accuracy: 0.1151 - val_loss: 2.2904 - val_accuracy: 0.1650\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1129 - val_loss: 2.2909 - val_accuracy: 0.1650\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2993 - accuracy: 0.1220 - val_loss: 2.2919 - val_accuracy: 0.1650\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2991 - accuracy: 0.1151 - val_loss: 2.2931 - val_accuracy: 0.1600\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2987 - accuracy: 0.1223 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2997 - accuracy: 0.1269 - val_loss: 2.2865 - val_accuracy: 0.1900\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3009 - accuracy: 0.1151 - val_loss: 2.2877 - val_accuracy: 0.1900\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2945 - accuracy: 0.1277 - val_loss: 2.2892 - val_accuracy: 0.1700\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2950 - accuracy: 0.1240 - val_loss: 2.2915 - val_accuracy: 0.1650\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.2972 - accuracy: 0.1217 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2958 - accuracy: 0.1323 - val_loss: 2.2796 - val_accuracy: 0.2000\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2957 - accuracy: 0.1263 - val_loss: 2.2815 - val_accuracy: 0.1900\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2952 - accuracy: 0.1303 - val_loss: 2.2844 - val_accuracy: 0.1900\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2909 - accuracy: 0.1300 - val_loss: 2.2890 - val_accuracy: 0.1650\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2881 - accuracy: 0.1449 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2884 - accuracy: 0.1383 - val_loss: 2.2606 - val_accuracy: 0.2500\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2852 - accuracy: 0.1471 - val_loss: 2.2636 - val_accuracy: 0.2300\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2759 - accuracy: 0.1683 - val_loss: 2.2710 - val_accuracy: 0.2100\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.2695 - accuracy: 0.1766 - val_loss: 2.2821 - val_accuracy: 0.1950\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2704 - accuracy: 0.1754 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2641 - accuracy: 0.1771 - val_loss: 2.1873 - val_accuracy: 0.3550\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2517 - accuracy: 0.2023 - val_loss: 2.1978 - val_accuracy: 0.3150\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2361 - accuracy: 0.2077 - val_loss: 2.2284 - val_accuracy: 0.2350\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2153 - accuracy: 0.2263 - val_loss: 2.2664 - val_accuracy: 0.1950\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2119 - accuracy: 0.2257 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.2159 - accuracy: 0.2249 - val_loss: 1.9922 - val_accuracy: 0.4350\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1703 - accuracy: 0.2460 - val_loss: 2.1162 - val_accuracy: 0.2550\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.1907 - accuracy: 0.2009 - val_loss: 2.2033 - val_accuracy: 0.3250\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1579 - accuracy: 0.2571 - val_loss: 2.2443 - val_accuracy: 0.1950\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1377 - accuracy: 0.2451 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1390 - accuracy: 0.2437 - val_loss: 1.5981 - val_accuracy: 0.6150\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0840 - accuracy: 0.2689 - val_loss: 1.8058 - val_accuracy: 0.3600\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0415 - accuracy: 0.2751 - val_loss: 2.0788 - val_accuracy: 0.3700\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.0231 - accuracy: 0.2911 - val_loss: 2.2064 - val_accuracy: 0.3050\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.9964 - accuracy: 0.3040 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0976 - accuracy: 0.2469 - val_loss: 1.7713 - val_accuracy: 0.3950\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0672 - accuracy: 0.2623 - val_loss: 1.5970 - val_accuracy: 0.4600\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.9744 - accuracy: 0.2954 - val_loss: 1.9908 - val_accuracy: 0.3000\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.8858 - accuracy: 0.3351 - val_loss: 2.1773 - val_accuracy: 0.2150\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.9538 - accuracy: 0.2880 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0038 - accuracy: 0.2694 - val_loss: 1.3729 - val_accuracy: 0.4800\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0091 - accuracy: 0.2766 - val_loss: 1.7232 - val_accuracy: 0.4950\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.8913 - accuracy: 0.3366 - val_loss: 1.9745 - val_accuracy: 0.4800\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.8624 - accuracy: 0.3323 - val_loss: 2.1551 - val_accuracy: 0.3500\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.8282 - accuracy: 0.3646 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1159 - accuracy: 0.2369 - val_loss: 1.8728 - val_accuracy: 0.3550\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0021 - accuracy: 0.3034 - val_loss: 1.3781 - val_accuracy: 0.6000\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1289 - accuracy: 0.2863 - val_loss: 2.2359 - val_accuracy: 0.0950\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0979 - accuracy: 0.2429 - val_loss: 2.2365 - val_accuracy: 0.0950\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.9753 - accuracy: 0.2869 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1534 - accuracy: 0.2200 - val_loss: 1.8600 - val_accuracy: 0.4500\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1246 - accuracy: 0.2360 - val_loss: 1.4674 - val_accuracy: 0.4800\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.0804 - accuracy: 0.2609 - val_loss: 2.0265 - val_accuracy: 0.3300\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1080 - accuracy: 0.2569 - val_loss: 2.2269 - val_accuracy: 0.2950\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1286 - accuracy: 0.2346 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2437 - accuracy: 0.1886 - val_loss: 1.9700 - val_accuracy: 0.2250\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1490 - accuracy: 0.2114 - val_loss: 1.9434 - val_accuracy: 0.4900\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.1353 - accuracy: 0.2254 - val_loss: 2.3796 - val_accuracy: 0.1400\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3558 - accuracy: 0.1686 - val_loss: 2.2829 - val_accuracy: 0.0900\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.2187 - accuracy: 0.1769 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3638 - accuracy: 0.1477 - val_loss: 2.3181 - val_accuracy: 0.1200\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.3937 - accuracy: 0.1209 - val_loss: 2.3410 - val_accuracy: 0.0800\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3637 - accuracy: 0.1243 - val_loss: 2.2861 - val_accuracy: 0.1250\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3266 - accuracy: 0.1377 - val_loss: 2.2862 - val_accuracy: 0.1000\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.0075 - accuracy: 0.1369 - val_loss: 2.2947 - val_accuracy: 0.1600\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 4.5133 - accuracy: 0.1089 - val_loss: 3.0915 - val_accuracy: 0.0850\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3709 - accuracy: 0.1109 - val_loss: 2.2202 - val_accuracy: 0.2650\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 4.1338 - accuracy: 0.1094 - val_loss: 2.3027 - val_accuracy: 0.0850\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.6814 - accuracy: 0.1257 - val_loss: 5.6356 - val_accuracy: 0.0850\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 78.7498 - accuracy: 0.0900 - val_loss: 2.2947 - val_accuracy: 0.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}